const { ollamaLLM } = require('./llmClient');
const { azureOAI } = require('./azureOpenaAIClient');
const { getParquetFiles, splitIntoBatches, shuffleInPlace,truncateWithTiktoken, writeJSONLStream  } = require('./utils');
const { Semaphore } = require('./semaphore');
const {
    generateInitialClustersPrompt,
    generateSummarizationPrompt,
    generateClusterUpdatePrompt,
    generateReviewPrompt,
    summaryJsonSchema,
    InitialClusterListSchema,
    UpdatedClusterListSchema
} = require('./clusterPrompts');

const path = require('path');

const SUMMARY_CONCURRENCY = parseInt(process.env.SUMMARY_CONCURRENCY || '4', 5);
const USE_CASE = "Primary area of interest. This will include the main topic of the conversation or any other specific subject that the user is interested in discussing or learning about.";
const MAX_CLUSTERS = 5; // maximum number of clusters to generate for the clustering task
const SUMMARY_LENGTH = 50 // maximum number of words for each summary generated by the LLM
const CLUSTER_NAME_LENGTH = 3; // maximum number of words for each cluster name generated by the LLM
const SUGGESTION_LIMIT = 20; // maximum number of suggestions to provide for new clusters during the clustering task

// Configuration flags for data processing
const SUMMARIZE_DATA = false; // set to false to skip summarization step if data is already concise enough
const TRUNCATE_DATA = true; // set to true to truncate each conversation to a max token limit before summarization
const DATA_LIMIT = 100; // max number of records to load from parquet files for clustering
const NUMBER_OF_BATCHES = 20; // max number of batches to split the data into for clustering



(async () => {

    // Step 1: Load chat data from parquet files
    const parquetData = await getParquetFiles(recordLimit=DATA_LIMIT);
    if (!parquetData.length) {
        console.error('No data loaded from parquet files.');
        // process.exit(1); // optional: leave running so other code paths can still execute
    }
    console.log("Parquet data loaded:", parquetData.length, "rows");


    // Optional step: truncate each message in the conversation to a max token limit
    // This helps to reduce the token count for LLM processing
    // Adjust the token limit as needed based on your use case and LLM constraints
    // Here we truncate to 250 tokens per conversation
    // You may skip summarization in next step if data is already concise enough

    let dataToProcess = parquetData
    if (TRUNCATE_DATA){
        console.log("Truncating each conversation to max token limit");
        dataToProcess = parquetData.map((chat) => {
        // truncate each message in the conversation to a max token limit
            const message = chat.conversation.map((m, i) => {
                let role;
                if (m.role) {
                const r = m.role.toLowerCase();
                role = (r === 'assistant' || r === 'ai' || r === 'model') ? 'AI' : 'User';
                } else {
                role = i % 2 === 0 ? 'User' : 'AI';
                }
                return `${role}: ${m.content}`;
            }).join('\n');
            const truncatedContent = truncateWithTiktoken(message, 250); // Adjust token limit as needed
            chat.summary = truncatedContent;
            return chat;
        })

    }
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    if (SUMMARIZE_DATA){
        
        // Step 2: Summarize each chat message using the LLM
        const semaphore = new Semaphore(SUMMARY_CONCURRENCY);

        const dataToProcessSummarized = await Promise.all(
        dataToProcess.map(chat =>
          semaphore.use(async () => {
            const message = chat.conversation.map((m, i) => {
                let role;
                if (m.role) {
                const r = m.role.toLowerCase();
                role = (r === 'assistant' || r === 'ai' || r === 'model') ? 'AI' : 'User';
                } else {
                role = i % 2 === 0 ? 'User' : 'AI';
                }
                return `${role}: ${m.content}`;
            }).join('\n');

            const summary = await azureOAI(generateSummarizationPrompt(data=message, usecase=USE_CASE, summaryLength=SUMMARY_LENGTH, customOutputFormat=""), summaryJsonSchema)

            let summaryOutput;
            // attempt to parse the summary response as JSON, or add custom logic based on output format if needed
            try {
                const summaryJsonResponse = JSON.parse(summary);
                summaryOutput = summaryJsonResponse.summary;
            } catch {
                summaryOutput = summary;
            }
            chat.summary = summaryOutput;
            console.log(`[summary] done id=${chat.id || chat.conversation_hash || '?'} | in-flight <= ${SUMMARY_CONCURRENCY}`);
            return chat;
          })
        )
        );
        writeJSONLStream(dataToProcessSummarized, path.join(__dirname, 'outputs', `summaries_${timestamp}.jsonl`));
        dataToProcess = dataToProcessSummarized
        console.log("Summarization completed for all data.");
    }
 

   

    // batch summarized data and iterate over each batch to perform clustering
    const batched_data = splitIntoBatches(shuffleInPlace(dataToProcess), NUMBER_OF_BATCHES)

    // generate a cluster for inital batch
    const initialBatch = batched_data[0]
    // generate seed clusters from initial batch
    const taxonomyGenerationPrompt = generateInitialClustersPrompt(data=initialBatch, usecase=USE_CASE, maxClusters=MAX_CLUSTERS, clusterNameLength=CLUSTER_NAME_LENGTH,  customOutputFormat="")

    const clusterTableResponse = await azureOAI(taxonomyGenerationPrompt, InitialClusterListSchema)
    console.log("Initial cluster response:", clusterTableResponse);

    
    // parse the cluster response
    let clusters = null
    try{
        clusters = JSON.parse(clusterTableResponse);
        console.log("Cluster response:", clusters);
    } catch (e) {
        console.error("Failed to parse cluster JSON:", e.message || e);
        clusters = null
    }

    // iterate over remaining batches and update cluster reference table
    for (const batch of batched_data.slice(1)) {
        // for each batch, assign each summary to one of the existing clusters
        if (!clusters) {
            console.error("No clusters available for assignment.");
            break;
        }

        // iteration prompt
        const clusteringPrompt = generateClusterUpdatePrompt(clusters, data=batch, maxClusters=MAX_CLUSTERS, useCase=USE_CASE,  suggestionLimit=SUGGESTION_LIMIT, customOutputFormat="")
        const clusterTableResponse = await azureOAI(clusteringPrompt, UpdatedClusterListSchema);
        
        console.log("Assignment response:", clusterTableResponse);

        try{
            clusters = JSON.parse(clusterTableResponse);
        } catch (e) {
            console.error("Failed to parse cluster JSON:", e.message || e);
        }

    }
    
    console.log("Final clusters before review:", clusters);

    // Taxonomy review:
  const reviewPrompt = generateReviewPrompt(clusters,maxClusters=MAX_CLUSTERS, useCase=USE_CASE, clusterNameLength=CLUSTER_NAME_LENGTH, suggestionLimit=SUGGESTION_LIMIT, customOutputFormat="");
  const finalTable = await azureOAI(reviewPrompt, UpdatedClusterListSchema);
  let finalClusterList = clusters
  try{
    finalClusterList = JSON.parse(finalTable);
  } catch (e) {
    console.error("Failed to parse cluster JSON:", e.message || e);
  }
  console.log("Final reviewed clusters:", finalClusterList);

  writeJSONLStream(finalClusterList["updatedTable"], path.join(__dirname, 'outputs', `finalClusterList_${timestamp}.jsonl`));



})();




